# ğŸ›¡ï¸ Byzantine-Resistant Federated Learning for Intrusion Detection (NSL-KDD)

This project simulates a **Byzantine-resilient Federated Learning (FL)** framework for building robust **Intrusion Detection Systems (IDS)** using the NSL-KDD dataset. It incorporates **malicious (Byzantine) clients** that send poisoned model updates and uses the **Krum aggregation algorithm** to defend against such attacks.

---

## ğŸš€ Highlights

- âš™ï¸ Federated Learning with multiple simulated clients.
- ğŸ” Byzantine fault tolerance using **Krum Aggregation**.
- ğŸ§  Local training using PyTorch neural networks.
- ğŸ“Š Real-time evaluation across global training rounds.
- ğŸŒ Scalable design for potential **real-time IDS deployment**.

---

## ğŸ“¥ Dataset: NSL-KDD
A curated version of the KDD Cup '99 dataset for IDS.
It contains labeled network traffic: normal, dos, probe, r2l, and u2r.

### Dataset Preprocessing:
- One-hot encoding for categorical features
- Standardization of continuous features  
- Label encoding for target classes
## ğŸ”§ Configurable Parameters
```python
NUM_CLIENTS = 5              # Total number of simulated clients
BYZANTINE_CLIENTS = 1        # Number of adversarial (malicious) clients
GLOBAL_ROUNDS = 10           # Number of communication rounds
EPOCHS = 5                   # Number of local training epochs
BATCH_SIZE = 32
LEARNING_RATE = 0.001
```


# ğŸ§  How It Works â€“ Step by Step

## 1. ğŸ“Š Data Preprocessing
- Load the NSL-KDD dataset  
- Clean and preprocess the data:
  - Remove non-numeric/unneeded fields
  - Encode categorical variables (e.g., `protocol_type`, `flag`)
  - Map fine-grained attack labels into broad categories
  - Normalize all feature values

## 2. ğŸ–¥ï¸ Federated Learning Setup
- Preprocessed training data is split across multiple clients
- Each client has access only to their local dataset (simulating real-world IDS data distribution)

## 3. âš ï¸ Byzantine Client Simulation
Some clients are labeled Byzantine (malicious) and:
- Flip model weights (sign-flip attack)
- Send constant/random weights to poison the global model

## 4. ğŸ‹ï¸ Local Model Training
- Non-Byzantine clients train their model on local data for `EPOCHS`
- Byzantine clients skip training and poison the model update

## 5. ğŸ›¡ï¸ Robust Aggregation (Krum)
Byzantine-resilient aggregation algorithm that:
1. Calculates distances between all model updates
2. Selects the update closest to most others (excluding outliers)

## 6. ğŸŒ Global Model Update
- The selected client model (via Krum) becomes the new global model
- All clients receive this updated model for next round

## 7. ğŸ“ˆ Evaluation
- Global model evaluated on hold-out test set after each round
- Test accuracy reported for each communication round

# ğŸŒ Real-World IDS Integration

## âœ… Scenario: Distributed Network Security
Each client represents different data sources:
- Firewall at remote branch
- Router at ISP hub  
- Endpoint protection system

## ğŸ” Why Federated Learning?
- **Privacy-preserving**: Only model updates shared, not raw traffic logs
- **Bandwidth efficient**: No massive packet data transfers
- **Real-time capable**: Periodic local training + off-peak aggregation

## ğŸ›¡ï¸ Robust to Compromise
- Compromised clients can't poison global model (thanks to Krum)
- Enhances resilience in production IDS systems

# ğŸ§° Production Setup Outline

**Local FL Clients** (deployed as agents):
- Collect/process real-time traffic
- Periodic local training
- Send weight updates to central server

**Central Server**:
- Runs Krum aggregation  
- Broadcasts updated global IDS model

# ğŸ§© Extension Opportunities
- Add more attack types and traffic logs
- Real-time data ingestion (Kafka/sockets)
- Integration with Suricata/Snort/Zeek
- Alternative robust aggregators:
  - Median
  - Trimmed Mean  
  - Bulyan

# ğŸ§ª Example Output
```arduino
--- Federated Round 1 ---
Client 1: Byzantine â€“ model poisoned
Client 2: Local training complete  
Client 3: Local training complete
Client 4: Local training complete
Client 5: Local training complete
Krum selected model from Client 3
Test Accuracy: 86.75%
```

#ğŸ“š References
-NSL-KDD Dataset

-Krum: Byzantine-Robust Aggregation

-Federated Learning â€“ Google AI Blog





